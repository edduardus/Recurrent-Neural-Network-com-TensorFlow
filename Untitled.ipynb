{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network com TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Para usar RNNs na classificação de imagens, precisamos considerar cada imagem como uma sequência de pixels. \n",
    "# Como as imagens no dataset MNIST possuem um shape 28x28 pixels, nós iremos trabalhar com 28 sequências de 28 timesteps para cada amostra.\n",
    "\n",
    "# Pacotes\n",
    "import input_data\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Dataset\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parâmetros para o processo de aprendizagem\n",
    "learning_rate = 0.001\n",
    "training_iters = 100000\n",
    "batch_size = 128\n",
    "display_step = 10\n",
    "\n",
    "# Inputs, Steps, Hidden e Output\n",
    "n_input = 28 \n",
    "n_steps = 28 \n",
    "n_hidden = 128 # número de features na camada oculta\n",
    "n_classes = 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input e classes\n",
    "x = tf.placeholder(\"float\", [None, n_steps, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# Pesos e bias\n",
    "weights = {'out': tf.Variable(tf.random_normal([n_hidden, n_classes]))}\n",
    "biases = {'out': tf.Variable(tf.random_normal([n_classes]))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Função para o modelo RNN\n",
    "def RNN(x, weights, biases):\n",
    "    x = tf.transpose(x, [1, 0, 2])\n",
    "    x = tf.reshape(x, [-1, n_input])\n",
    "    x = tf.split(axis = 0, num_or_size_splits = n_steps, value = x)\n",
    "    lstm_cell = rnn.BasicLSTMCell(n_hidden, forget_bias = 1.0)\n",
    "    outputs, states = rnn.static_rnn(lstm_cell, x, dtype = tf.float32)\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-a2b8f47dc292>:5: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Executando o modelo\n",
    "pred = RNN(x, weights, biases)\n",
    "\n",
    "# Cost Function e Otimização\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = pred, labels = y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "\n",
    "# Previsões e acurácia\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Inicializando as variáveis\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteração 1280, Perda = 1.851419, Acurácia no Treino = 0.37500\n",
      "Iteração 2560, Perda = 1.408448, Acurácia no Treino = 0.54688\n",
      "Iteração 3840, Perda = 1.361712, Acurácia no Treino = 0.57031\n",
      "Iteração 5120, Perda = 1.195803, Acurácia no Treino = 0.62500\n",
      "Iteração 6400, Perda = 0.955298, Acurácia no Treino = 0.67969\n",
      "Iteração 7680, Perda = 0.800733, Acurácia no Treino = 0.71875\n",
      "Iteração 8960, Perda = 0.655397, Acurácia no Treino = 0.78125\n",
      "Iteração 10240, Perda = 0.517085, Acurácia no Treino = 0.81250\n",
      "Iteração 11520, Perda = 0.498177, Acurácia no Treino = 0.78906\n",
      "Iteração 12800, Perda = 0.372346, Acurácia no Treino = 0.90625\n",
      "Iteração 14080, Perda = 0.339670, Acurácia no Treino = 0.88281\n",
      "Iteração 15360, Perda = 0.378078, Acurácia no Treino = 0.91406\n",
      "Iteração 16640, Perda = 0.354885, Acurácia no Treino = 0.88281\n",
      "Iteração 17920, Perda = 0.263813, Acurácia no Treino = 0.92969\n",
      "Iteração 19200, Perda = 0.350499, Acurácia no Treino = 0.90625\n",
      "Iteração 20480, Perda = 0.406440, Acurácia no Treino = 0.87500\n",
      "Iteração 21760, Perda = 0.302922, Acurácia no Treino = 0.85938\n",
      "Iteração 23040, Perda = 0.238980, Acurácia no Treino = 0.91406\n",
      "Iteração 24320, Perda = 0.205099, Acurácia no Treino = 0.94531\n",
      "Iteração 25600, Perda = 0.336521, Acurácia no Treino = 0.91406\n",
      "Iteração 26880, Perda = 0.411741, Acurácia no Treino = 0.89844\n",
      "Iteração 28160, Perda = 0.372374, Acurácia no Treino = 0.85938\n",
      "Iteração 29440, Perda = 0.314651, Acurácia no Treino = 0.90625\n",
      "Iteração 30720, Perda = 0.380511, Acurácia no Treino = 0.90625\n",
      "Iteração 32000, Perda = 0.255480, Acurácia no Treino = 0.90625\n",
      "Iteração 33280, Perda = 0.163153, Acurácia no Treino = 0.95312\n",
      "Iteração 34560, Perda = 0.273800, Acurácia no Treino = 0.89844\n",
      "Iteração 35840, Perda = 0.149031, Acurácia no Treino = 0.96875\n",
      "Iteração 37120, Perda = 0.397979, Acurácia no Treino = 0.89062\n",
      "Iteração 38400, Perda = 0.198946, Acurácia no Treino = 0.95312\n",
      "Iteração 39680, Perda = 0.277419, Acurácia no Treino = 0.92969\n",
      "Iteração 40960, Perda = 0.140826, Acurácia no Treino = 0.94531\n",
      "Iteração 42240, Perda = 0.172388, Acurácia no Treino = 0.95312\n",
      "Iteração 43520, Perda = 0.299182, Acurácia no Treino = 0.92969\n",
      "Iteração 44800, Perda = 0.235060, Acurácia no Treino = 0.93750\n",
      "Iteração 46080, Perda = 0.142909, Acurácia no Treino = 0.96875\n",
      "Iteração 47360, Perda = 0.105389, Acurácia no Treino = 0.97656\n",
      "Iteração 48640, Perda = 0.081376, Acurácia no Treino = 0.97656\n",
      "Iteração 49920, Perda = 0.159579, Acurácia no Treino = 0.93750\n",
      "Iteração 51200, Perda = 0.133451, Acurácia no Treino = 0.96094\n",
      "Iteração 52480, Perda = 0.129865, Acurácia no Treino = 0.96875\n",
      "Iteração 53760, Perda = 0.119597, Acurácia no Treino = 0.96094\n",
      "Iteração 55040, Perda = 0.211887, Acurácia no Treino = 0.94531\n",
      "Iteração 56320, Perda = 0.114926, Acurácia no Treino = 0.94531\n",
      "Iteração 57600, Perda = 0.135319, Acurácia no Treino = 0.96094\n",
      "Iteração 58880, Perda = 0.113320, Acurácia no Treino = 0.95312\n",
      "Iteração 60160, Perda = 0.202113, Acurácia no Treino = 0.92188\n",
      "Iteração 61440, Perda = 0.078989, Acurácia no Treino = 0.97656\n",
      "Iteração 62720, Perda = 0.116103, Acurácia no Treino = 0.95312\n",
      "Iteração 64000, Perda = 0.097362, Acurácia no Treino = 0.98438\n",
      "Iteração 65280, Perda = 0.118982, Acurácia no Treino = 0.95312\n",
      "Iteração 66560, Perda = 0.177573, Acurácia no Treino = 0.93750\n",
      "Iteração 67840, Perda = 0.123056, Acurácia no Treino = 0.96094\n",
      "Iteração 69120, Perda = 0.237944, Acurácia no Treino = 0.94531\n",
      "Iteração 70400, Perda = 0.063684, Acurácia no Treino = 0.97656\n",
      "Iteração 71680, Perda = 0.142928, Acurácia no Treino = 0.95312\n",
      "Iteração 72960, Perda = 0.116359, Acurácia no Treino = 0.95312\n",
      "Iteração 74240, Perda = 0.100340, Acurácia no Treino = 0.97656\n",
      "Iteração 75520, Perda = 0.128014, Acurácia no Treino = 0.96094\n",
      "Iteração 76800, Perda = 0.127224, Acurácia no Treino = 0.95312\n",
      "Iteração 78080, Perda = 0.074276, Acurácia no Treino = 0.97656\n",
      "Iteração 79360, Perda = 0.085146, Acurácia no Treino = 0.96094\n",
      "Iteração 80640, Perda = 0.184252, Acurácia no Treino = 0.95312\n",
      "Iteração 81920, Perda = 0.082698, Acurácia no Treino = 0.98438\n",
      "Iteração 83200, Perda = 0.118142, Acurácia no Treino = 0.95312\n",
      "Iteração 84480, Perda = 0.097889, Acurácia no Treino = 0.97656\n",
      "Iteração 85760, Perda = 0.082882, Acurácia no Treino = 0.96875\n",
      "Iteração 87040, Perda = 0.122614, Acurácia no Treino = 0.96094\n",
      "Iteração 88320, Perda = 0.157928, Acurácia no Treino = 0.95312\n",
      "Iteração 89600, Perda = 0.197088, Acurácia no Treino = 0.95312\n",
      "Iteração 90880, Perda = 0.056006, Acurácia no Treino = 0.96875\n",
      "Iteração 92160, Perda = 0.159703, Acurácia no Treino = 0.95312\n",
      "Iteração 93440, Perda = 0.064813, Acurácia no Treino = 0.98438\n",
      "Iteração 94720, Perda = 0.140678, Acurácia no Treino = 0.94531\n",
      "Iteração 96000, Perda = 0.068626, Acurácia no Treino = 0.98438\n",
      "Iteração 97280, Perda = 0.039879, Acurácia no Treino = 0.98438\n",
      "Iteração 98560, Perda = 0.066801, Acurácia no Treino = 0.97656\n",
      "Iteração 99840, Perda = 0.088930, Acurácia no Treino = 0.97656\n",
      "Treinamento e Otimização Concluídos!\n",
      "Acurácia no Teste: 0.960938\n"
     ]
    }
   ],
   "source": [
    "# Sessão\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step = 1\n",
    "    while step * batch_size < training_iters:\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        batch_x = batch_x.reshape((batch_size, n_steps, n_input))\n",
    "        sess.run(optimizer, feed_dict = {x: batch_x, y: batch_y})\n",
    "        if step % display_step == 0:\n",
    "            acc = sess.run(accuracy, feed_dict = {x: batch_x, y: batch_y})\n",
    "            loss = sess.run(cost, feed_dict = {x: batch_x, y: batch_y})\n",
    "            print(\"Iteração \" + str(step*batch_size) + \", Perda = \" + \"{:.6f}\".format(loss) + \", Acurácia no Treino = \" + \"{:.5f}\".format(acc))\n",
    "        step += 1\n",
    "    print(\"Treinamento e Otimização Concluídos!\")\n",
    "    test_len = 128\n",
    "    test_data = mnist.test.images[:test_len].reshape((-1, n_steps, n_input))\n",
    "    test_label = mnist.test.labels[:test_len]\n",
    "    print(\"Acurácia no Teste:\", sess.run(accuracy, feed_dict = {x: test_data, y: test_label}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
